## SMAI - Implementations

This repository consists of assignments done as a part of the SMAI course done in Monsoon'23 semester.

The datasets used have not been included in this repository.

## Table of Contents

1. [Part-1: k-Nearest Neighbours and Decision Trees](#part-1-k-nearest-neighbours-and-decision-trees)
2. [Part-2: PCA, Gaussian Mixture Models, and Clustering](#part-2-pca-gaussian-mixture-models-and-clustering)
3. [Part-3: Linear Models, MLPs, CNNs, and Autoencoders](#part-3-linear-models-mlps-cnns-and-autoencoders)
4. [Part-4: Ensemble Methods](#part-4-ensemble-methods)

## Part-1: k-Nearest Neighbours and Decision Trees

- **k-Nearest Neighbours (k-NN)**: Implemented the k-NN algorithm from scratch, focusing on optimizing the algorithm through vectorization techniques to enhance computational efficiency. 
  
- **Decision Trees**: Explored the powerset and multi-output formulations of classification using decision trees. Explored the flexibility of decision trees in handling complex classification problems with multiple outputs, providing insights into the interpretability and structure of tree-based models.

## Part-2: PCA, Gaussian Mixture Models, and Clustering

- **Principal Component Analysis (PCA)**: Implemented PCA from scratch, emphasizing its role in dimensionality reduction and feature extraction. 
  
- **Gaussian Mixture Models (GMMs)**: Developed the Expectation-Maximization algorithm (A probabilistic algorithm) for GMMs from the ground up.
  
- **Hierarchical Clustering**: Conducted ablation studies on hierarchical clustering, providing insights into the clustering process and its sensitivity to different linkage criteria and distance metrics. 
## Part-3: Linear Models, MLPs, CNNs, and Autoencoders

- **Multinomial Linear and Logistic Regression**: Implemented these foundational models from scratch, focusing on their application to multi-class classification problems.
  
- **Multilayer Perceptrons (MLPs)**: Built MLPs from the ground up, exploring the role of hidden layers and activation functions in deep learning. 
  
- **Convolutional Neural Networks (CNNs) and Autoencoders**: Leveraged inbuilt Torch modules to implement CNNs and autoencoders, focusing on their application to image processing tasks. This work highlights the importance of convolutional operations in feature extraction and the role of autoencoders in unsupervised learning and dimensionality reduction.

## Part-4: Ensemble Methods

- **Bagging and Stacking**: Implemented Bagging and Stacking algorithms from scratch, demonstrating the power of ensemble learning in reducing variance and improving predictive performance.

- **Random Forest, AdaBoost, and Gradient Boosted Decision Trees**: Developed these powerful ensemble methods, emphasizing their application to a variety of machine learning tasks. 
